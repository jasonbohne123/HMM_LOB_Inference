{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximization of the Log-Likelihood of Hidden Markov Models on the Limit Order Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 21612 consecutive repeated values from input series\n",
      "Dropped 18853 consecutive repeated values from input series\n",
      "Dropped 9378 consecutive repeated values from input series\n",
      "Dropped 13603 consecutive repeated values from input series\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bid_Size</th>\n",
       "      <th>Offer_Size</th>\n",
       "      <th>spread</th>\n",
       "      <th>OB_IB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:30:00.134062</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342176</td>\n",
       "      <td>0.430041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:30:00.134336</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330141</td>\n",
       "      <td>0.430041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:30:00.134532</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091578</td>\n",
       "      <td>0.372143</td>\n",
       "      <td>0.482240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:30:00.136081</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336317</td>\n",
       "      <td>0.430041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:30:00.234474</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380608</td>\n",
       "      <td>0.430041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:57:39.650647</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212638</td>\n",
       "      <td>0.158344</td>\n",
       "      <td>0.551244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:57:39.650839</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183157</td>\n",
       "      <td>0.158344</td>\n",
       "      <td>0.534440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:57:39.650865</th>\n",
       "      <td>0.121383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204657</td>\n",
       "      <td>0.377841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:57:39.651071</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091578</td>\n",
       "      <td>0.158344</td>\n",
       "      <td>0.482240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:57:39.651189</th>\n",
       "      <td>0.121383</td>\n",
       "      <td>0.183157</td>\n",
       "      <td>0.158344</td>\n",
       "      <td>0.482240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Bid_Size  Offer_Size    spread     OB_IB\n",
       "2020-01-02 09:30:00.134062  0.000000    0.000000  0.342176  0.430041\n",
       "2020-01-02 09:30:00.134336  0.000000    0.000000  0.330141  0.430041\n",
       "2020-01-02 09:30:00.134532  0.000000    0.091578  0.372143  0.482240\n",
       "2020-01-02 09:30:00.136081  0.000000    0.000000  0.336317  0.430041\n",
       "2020-01-02 09:30:00.234474  0.000000    0.000000  0.380608  0.430041\n",
       "...                              ...         ...       ...       ...\n",
       "2020-01-02 09:57:39.650647  0.000000    0.212638  0.158344  0.551244\n",
       "2020-01-02 09:57:39.650839  0.000000    0.183157  0.158344  0.534440\n",
       "2020-01-02 09:57:39.650865  0.121383    0.000000  0.204657  0.377841\n",
       "2020-01-02 09:57:39.651071  0.000000    0.091578  0.158344  0.482240\n",
       "2020-01-02 09:57:39.651189  0.121383    0.183157  0.158344  0.482240\n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=pd.read_csv('data/features.csv',index_col=0,nrows=50000)\n",
    "\n",
    "def remove_duplicates(series):\n",
    "    \n",
    "    cleaned_series=series[np.insert(np.diff(series).astype(bool), 0, True)]\n",
    "    dropped_els=len(series)-len(cleaned_series)\n",
    "    \n",
    "    print(f\"Dropped {dropped_els} consecutive repeated values from input series\")\n",
    "    return cleaned_series\n",
    "\n",
    "bidsize=remove_duplicates(features['Bid_Size'].values)\n",
    "offersize=remove_duplicates(features['Offer_Size'].values)\n",
    "bookimbalance=remove_duplicates(features['OB_IB'].values)\n",
    "spread=remove_duplicates(features['spread'].values)\n",
    "\n",
    "# formatted as numpy float \n",
    "np.savetxt(r'psg_example_hmm/vector_bidsize.txt', bidsize)\n",
    "np.savetxt(r'psg_example_hmm/vector_offersize.txt', offersize)\n",
    "np.savetxt(r'psg_example_hmm/vector_bookimbalance.txt', bookimbalance)\n",
    "np.savetxt(r'psg_example_hmm/vector_spread.txt', spread)\n",
    "\n",
    "features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSG\n",
    "\n",
    "- Formatting as a valid input type for PSG function hmm_discrete(x,0)\n",
    "- States x\n",
    "    - Spread Widens\n",
    "    - Spread Narrows\n",
    "\n",
    "- Observations o\n",
    "    - Will need to pass one feature vector in at a time \n",
    "    -   4 x m matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.add_dll_directory('C:\\Aorda\\PSG\\lib')\n",
    "import psgpython as psg \n",
    "from psg_loader import load_psg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_psg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Problem Imported\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psg_prob = psg.psg_importfromtext('./psg_example_hmm/problem_hmm_discrete.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_prob['problem_statement'] = '\\n'.join(psg_prob['problem_statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running solver\n",
      "Reading problem formulation\n",
      "Warning: 1 Object: Problem analizing\tMessage: Problem Statement: objective may be non-convex\n",
      "Asking for data information\n",
      "Getting data\n",
      "    100.0% of scenarios is processed\n",
      "100% of vector_bookimbalance was read\n",
      "Start optimization\n",
      "Ext.iteration=0  Objective=0.521629177915E+00  Residual=0.000000000000E+00\n",
      "Ext.iteration=10  Objective=0.521629177915E+00  Residual=0.000000000000E+00\n",
      "Optimization is stopped\n",
      "Solution is optimal\n",
      "Calculating resulting outputs. Writing solution.\n",
      "Objective: objective = 52191.9456035 [-1.042246337158E+17]\n",
      "Solver has normally finished. Solution was saved.\n",
      "Problem: problem_hmm_normal, solution_status = optimal\n",
      "Timing: data_loading_time = 0.09, preprocessing_time = 26.62, solving_time = 0.65\n",
      "Variables: optimal_point = point_problem_hmm_normal\n",
      "Objective: objective = 52191.9456035 [-1.042246337158E+17]\n",
      "Constraint: sum_of_probabilities_for_states = vector_sum_of_probabilities_for_states\n",
      "Function: hmm_normal(2,vector_bookimbalance) =  5.219194560347E+04\n",
      "OK. Solver Finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solution=psg.psg_solver(psg_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['problem_hmm_normal', 'optimal', ['problem_HMM_Normal, maximize', '  hmm_normal(2,vector_bookimbalance)', '  Solver: VAN'], ['Problem: problem_hmm_normal, solution_status = optimal', 'Timing: data_loading_time = 0.09, preprocessing_time = 26.62, solving_time = 0.65', 'Variables: optimal_point = point_problem_hmm_normal', 'Objective: objective = 52191.9456035 [-1.042246337158E+17]', 'Constraint: sum_of_probabilities_for_states = vector_sum_of_probabilities_for_states', 'Function: hmm_normal(2,vector_bookimbalance) =  5.219194560347E+04'], [['p1', 'p2', 'a1_1', 'a1_2', 'a2_1', 'a2_2', 'mu1', 'si1', 'mu2', 'si2'], array([0.        , 1.        , 0.89950989, 0.10049011, 0.01105765,\n",
       "       0.98894235, 0.4419428 , 0.13609657, 0.43252718, 0.05953872])], array([2., 2., 2., ..., 2., 2., 2.]), array([1., 1., 1.]), array([0.00000000e+00, 1.75415238e-14, 1.93178806e-14]), [['state1', 'state2'], array([[0.        , 1.        ],\n",
       "       [0.00135759, 0.99864241],\n",
       "       [0.00195491, 0.99804509],\n",
       "       ...,\n",
       "       [0.04711161, 0.95288839],\n",
       "       [0.04045023, 0.95954977],\n",
       "       [0.04036488, 0.95963512]])], ['Warning: 1 Object: Problem analizing\\tMessage: Problem Statement: objective may be non-convex'], ['Reading problem formulation', 'Warning: 1 Object: Problem analizing\\tMessage: Problem Statement: objective may be non-convex', 'Asking for data information', 'Getting data', '    100.0% of scenarios is processed', '100% of vector_bookimbalance was read', 'Start optimization', 'Ext.iteration=0  Objective=0.521629177915E+00  Residual=0.000000000000E+00', 'Ext.iteration=10  Objective=0.521629177915E+00  Residual=0.000000000000E+00', 'Optimization is stopped', 'Solution is optimal', 'Calculating resulting outputs. Writing solution.', 'Objective: objective = 52191.9456035 [-1.042246337158E+17]', 'Solver has normally finished. Solution was saved.']])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM Learn\n",
    "\n",
    "- Train HMM on one feature at a time\n",
    "- Assume each feature is normally distributed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 -1  1 -1  1  1  0  1 -1 -1  0  1  1 -1 -1  1  1  1 -1  1 -1  1  0  0\n",
      "  1 -1  0  0  1 -1 -1  1  1 -1 -1  1 -1  1 -1  0  0  0 -1  1  1 -1  1 -1\n",
      " -1  1  1  1 -1  1 -1  0 -1  1 -1  0  0  0  0  0  0  0  1 -1  1 -1 -1  1\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1 -1  0  0  0  1 -1  0  1 -1  0  0  0  0  0  0  1 -1  0  1 -1\n",
      " -1  1  1  0  0 -1  0  0  0  1  0 -1  1 -1  0  1 -1  0  0  1 -1  1  1 -1\n",
      "  1  1 -1  1 -1  1 -1  0  1 -1  1 -1  0  1 -1  1  1  0 -1  1  0  0 -1  1\n",
      "  0  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbohn\\AppData\\Local\\Temp\\ipykernel_11220\\2129338783.py:14: DeprecationWarning: less that 170 samples in lengths array [ 0 -1  1 -1  1  1  0  1 -1 -1  0  1  1 -1 -1  1  1  1 -1  1 -1  1  0  0\n",
      "  1 -1  0  0  1 -1 -1  1  1 -1 -1  1 -1  1 -1  0  0  0 -1  1  1 -1  1 -1\n",
      " -1  1  1  1 -1  1 -1  0 -1  1 -1  0  0  0  0  0  0  0  1 -1  1 -1 -1  1\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1 -1  0  0  0  1 -1  0  1 -1  0  0  0  0  0  0  1 -1  0  1 -1\n",
      " -1  1  1  0  0 -1  0  0  0  1  0 -1  1 -1  0  1 -1  0  0  1 -1  1  1 -1\n",
      "  1  1 -1  1 -1  1 -1  0  1 -1  1 -1  0  1 -1  1  1  0 -1  1  0  0 -1  1\n",
      "  0  1]; support for silently dropping samples is deprecated and will be removed\n",
      "  fitted_spread=spread_model.fit(X_train,y_train)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Aorda\\PSG\\Python\\Examples\\HMM\\hmm.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Aorda/PSG/Python/Examples/HMM/hmm.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m spread_model\u001b[39m=\u001b[39mGaussianHMM(n_components\u001b[39m=\u001b[39mspread_states,covariance_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdiag\u001b[39m\u001b[39m'\u001b[39m,startprob_prior\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, transmat_prior\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, algorithm\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mviterbi\u001b[39m\u001b[39m'\u001b[39m,params\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstmc\u001b[39m\u001b[39m'\u001b[39m, init_params\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstmc\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Aorda/PSG/Python/Examples/HMM/hmm.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m spread_model\u001b[39m.\u001b[39mstartprob_ \u001b[39m=\u001b[39m X_train[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Aorda/PSG/Python/Examples/HMM/hmm.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m fitted_spread\u001b[39m=\u001b[39mspread_model\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Aorda/PSG/Python/Examples/HMM/hmm.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m log_prob\u001b[39m=\u001b[39mfitted_spread\u001b[39m.\u001b[39mscore(X_test,y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Aorda/PSG/Python/Examples/HMM/hmm.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOut of Sample Log Probability is \u001b[39m\u001b[39m{\u001b[39;00mlog_prob\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jbohn\\AppData\\Local\\R-MINI~1\\envs\\hmm\\lib\\site-packages\\hmmlearn\\base.py:515\u001b[0m, in \u001b[0;36mBaseHMM.fit\u001b[1;34m(self, X, lengths)\u001b[0m\n\u001b[0;32m    510\u001b[0m     lattice, log_prob, posteriors, fwdlattice, bwdlattice \u001b[39m=\u001b[39m \\\n\u001b[0;32m    511\u001b[0m             impl(sub_X)\n\u001b[0;32m    512\u001b[0m     \u001b[39m# Derived HMM classes will implement the following method to\u001b[39;00m\n\u001b[0;32m    513\u001b[0m     \u001b[39m# update their probability distributions, so keep\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[39m# a single call to this method for simplicity.\u001b[39;00m\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accumulate_sufficient_statistics(\n\u001b[0;32m    516\u001b[0m         stats, sub_X, lattice, posteriors, fwdlattice,\n\u001b[0;32m    517\u001b[0m         bwdlattice)\n\u001b[0;32m    518\u001b[0m     curr_log_prob \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m log_prob\n\u001b[0;32m    520\u001b[0m \u001b[39m# XXX must be before convergence check, because otherwise\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[39m#     there won't be any updates for the case ``n_iter=1``.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbohn\\AppData\\Local\\R-MINI~1\\envs\\hmm\\lib\\site-packages\\hmmlearn\\hmm.py:246\u001b[0m, in \u001b[0;36mGaussianHMM._accumulate_sufficient_statistics\u001b[1;34m(self, stats, obs, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_accumulate_sufficient_statistics\u001b[39m(\u001b[39mself\u001b[39m, stats, obs, lattice,\n\u001b[0;32m    245\u001b[0m                                       posteriors, fwdlattice, bwdlattice):\n\u001b[1;32m--> 246\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_accumulate_sufficient_statistics(\n\u001b[0;32m    247\u001b[0m         stats, obs, lattice, posteriors, fwdlattice, bwdlattice)\n\u001b[0;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams:\n\u001b[0;32m    250\u001b[0m         stats[\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m posteriors\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jbohn\\AppData\\Local\\R-MINI~1\\envs\\hmm\\lib\\site-packages\\hmmlearn\\base.py:769\u001b[0m, in \u001b[0;36mBaseHMM._accumulate_sufficient_statistics\u001b[1;34m(self, stats, X, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39mUpdate sufficient statistics from a given sample.\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39m    forward and backward probabilities.\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    764\u001b[0m impl \u001b[39m=\u001b[39m {\n\u001b[0;32m    765\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mscaling\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulate_sufficient_statistics_scaling,\n\u001b[0;32m    766\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulate_sufficient_statistics_log,\n\u001b[0;32m    767\u001b[0m }[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimplementation]\n\u001b[1;32m--> 769\u001b[0m \u001b[39mreturn\u001b[39;00m impl(stats\u001b[39m=\u001b[39;49mstats, X\u001b[39m=\u001b[39;49mX, lattice\u001b[39m=\u001b[39;49mlattice, posteriors\u001b[39m=\u001b[39;49mposteriors,\n\u001b[0;32m    770\u001b[0m             fwdlattice\u001b[39m=\u001b[39;49mfwdlattice, bwdlattice\u001b[39m=\u001b[39;49mbwdlattice)\n",
      "File \u001b[1;32mc:\\Users\\jbohn\\AppData\\Local\\R-MINI~1\\envs\\hmm\\lib\\site-packages\\hmmlearn\\base.py:799\u001b[0m, in \u001b[0;36mBaseHMM._accumulate_sufficient_statistics_log\u001b[1;34m(self, stats, X, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[0;32m    797\u001b[0m stats[\u001b[39m'\u001b[39m\u001b[39mnobs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    798\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams:\n\u001b[1;32m--> 799\u001b[0m     stats[\u001b[39m'\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m posteriors[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    800\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams:\n\u001b[0;32m    801\u001b[0m     n_samples, n_components \u001b[39m=\u001b[39m lattice\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "\n",
    "counter=0\n",
    "for train_index, test_index in tscv.split(features):\n",
    "    \n",
    "    X_train, X_test = features.iloc[train_index].values, features.iloc[test_index].values\n",
    "    y_train, y_test = outcomes['spread_state'].iloc[train_index].values, outcomes['spread_state'].iloc[test_index].values\n",
    "    print(y_train)\n",
    "    spread_states=3\n",
    "\n",
    "    spread_model=GaussianHMM(n_components=spread_states,covariance_type='diag',startprob_prior=1.0, transmat_prior=1.0, algorithm='viterbi',params='stmc', init_params='stmc')\n",
    "    spread_model.startprob_ = X_train[0]\n",
    "    fitted_spread=spread_model.fit(X_train,y_train)\n",
    "    log_prob=fitted_spread.score(X_test,y_test)\n",
    "    print(f\"Out of Sample Log Probability is {log_prob}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('hmm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80cc58d642badbeb27aa4dea55477ea0db095f88739fbd6a40ddef87b59ea8ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
